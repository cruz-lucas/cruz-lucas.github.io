<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-wiki/machine-learning/ml-systems/chapter-6" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.5.2">
<title data-rh="true">6. Model Development and Offline Evaluation | Lucas Cruz</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://lucas-cruz.com/ml-systems/chapter-6"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="6. Model Development and Offline Evaluation | Lucas Cruz"><meta data-rh="true" name="description" content="Six Tips for Model Selection"><meta data-rh="true" property="og:description" content="Six Tips for Model Selection"><link data-rh="true" rel="icon" href="/img/favicon/favicon.ico"><link data-rh="true" rel="canonical" href="https://lucas-cruz.com/ml-systems/chapter-6"><link data-rh="true" rel="alternate" href="https://lucas-cruz.com/ml-systems/chapter-6" hreflang="en"><link data-rh="true" rel="alternate" href="https://lucas-cruz.com/ml-systems/chapter-6" hreflang="x-default"><link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-MS3FF7B6JV"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-MS3FF7B6JV",{anonymize_ip:!0})</script>
<link rel="preconnect" href="https://www.googletagmanager.com">
<script>window.dataLayer=window.dataLayer||[]</script>
<script>!function(e,t,a,n){e[n]=e[n]||[],e[n].push({"gtm.start":(new Date).getTime(),event:"gtm.js"});var g=t.getElementsByTagName(a)[0],m=t.createElement(a);m.async=!0,m.src="https://www.googletagmanager.com/gtm.js?id=GTM-57WJMXBB",g.parentNode.insertBefore(m,g)}(window,document,"script","dataLayer")</script>



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.4c50bf2e.css">
<script src="/assets/js/runtime~main.a9c3a612.js" defer="defer"></script>
<script src="/assets/js/main.efa2a684.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-57WJMXBB" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>

<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" target="_self" href="/"><div class="navbar__logo"><img src="/img/favicon/favicon-32x32.png" alt="Site Logo" class="custom-navbar-logo-class themedComponent_mlkZ themedComponent--light_NVdE" height="32" width="32"><img src="/img/favicon/dark-favicon-32x32.png" alt="Site Logo" class="custom-navbar-logo-class themedComponent_mlkZ themedComponent--dark_xIcU" height="32" width="32"></div><b class="navbar__title text--truncate">Lucas Cruz</b></a><div class="navbar__item dropdown dropdown--hoverable"><a class="navbar__link" aria-haspopup="true" aria-expanded="false" role="button" href="/wiki">Wiki</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/rl">Reinforcement Learning</a></li><li><a aria-current="page" class="dropdown__link dropdown__link--active" href="/ml-systems">Machine Learning</a></li><li><a class="dropdown__link" href="/statistics">Statistics</a></li></ul></div></div><div class="navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/cv">Curriculum Vitae</a><a href="https://github.com/cruz-lucas" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-github-link" aria-label="GitHub repository"></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" href="/ml-systems">Designing Machine Learning Systems</a><button aria-label="Collapse sidebar category &#x27;Designing Machine Learning Systems&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ml-systems/chapter-1">1. Overview of Machine Learning Systems</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ml-systems/chapter-2">2. Introduction to Machine Learning Systems Design</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ml-systems/chapter-3">3. Data Engineering Fundamentals</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ml-systems/chapter-4">4. Training Data</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ml-systems/chapter-5">5. Feature Engineering</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/ml-systems/chapter-6">6. Model Development and Offline Evaluation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ml-systems/chapter-7">7. Model Deployment and Prediction Service</a></li></ul></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/ml-systems"><span itemprop="name">Designing Machine Learning Systems</span></a><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">6. Model Development and Offline Evaluation</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Model Development and Offline Evaluation</h1></header>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="six-tips-for-model-selection">Six Tips for Model Selection<a href="#six-tips-for-model-selection" class="hash-link" aria-label="Direct link to Six Tips for Model Selection" title="Direct link to Six Tips for Model Selection">​</a></h2>
<ol>
<li>
<p><strong>Avoid the state-of-the-art trap:</strong> Don’t be swayed by the latest and most complex algorithms just because they are state-of-the-art. Often, simpler models can perform just as well or better, especially if they are well-tuned and well-understood. Remember that researchers often evaluate models in academic settings, which we discussed in Chapter 1.</p>
</li>
<li>
<p><strong>Start with the simplest models:</strong> Begin with simple models like linear regression or decision trees. These models are easier to interpret and debug. Once you establish a baseline performance and ensure your training and prediction pipelines are consistent, you can move to more complex models if necessary. While you can start with more complex models that require little effort to get started (e.g., a pretrained version of BERT from Hugging Face&#x27;s Transformers), always test simpler models to verify that the more complex solution indeed outperforms them.</p>
<blockquote>
<p>Simple is better than complex</p>
</blockquote>
</li>
<li>
<p><strong>Avoid human biases in selecting models:</strong> Ensure that model selection is based on objective performance metrics rather than subjective preferences or biases. If an engineer is more enthusiastic about a specific solution, they may spend more time tuning it. Make sure to compare architectures under similar setups.</p>
</li>
<li>
<p><strong>Evaluate good performance now versus good performance later:</strong> Consider both short-term and long-term performance. Some models might perform well initially but degrade over time, while others might improve as more data is collected. Regularly monitor model performance and be prepared to update or replace models as necessary.</p>
</li>
<li>
<p><strong>Evaluate trade-offs:</strong> Every model comes with trade-offs. Consider factors such as training time, inference time, scalability, interpretability, and resource requirements, as well as the trade-off between false positives and false negatives. Choose a model that balances these factors in a way that aligns with your project goals.</p>
</li>
<li>
<p><strong>Understand your model&#x27;s assumptions:</strong> Each model makes specific assumptions about the data. Ensure that these assumptions hold for your dataset. For example:</p>
<ol>
<li><em>Prediction assumption:</em> It&#x27;s possible to predict <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi></mrow><annotation encoding="application/x-tex">Y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.22222em">Y</span></span></span></span> based on <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07847em">X</span></span></span></span>.</li>
<li><em>IID:</em> Neural Networks assume that examples are independent and identically distributed.</li>
<li><em>Smoothness:</em> If an input <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07847em">X</span></span></span></span> produces an output <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi></mrow><annotation encoding="application/x-tex">Y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.22222em">Y</span></span></span></span>, then an input close to <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07847em">X</span></span></span></span> would produce an output proportionally close to <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi></mrow><annotation encoding="application/x-tex">Y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.22222em">Y</span></span></span></span>.</li>
<li><em>Tractability:</em> Let <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07847em">X</span></span></span></span> be the input and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Z</mi></mrow><annotation encoding="application/x-tex">Z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07153em">Z</span></span></span></span> be the latent representation of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07847em">X</span></span></span></span>. Every generative model makes the assumption that it&#x27;s tractable to compute the probability <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="double-struck">P</mi><mo stretchy="false">(</mo><mi>Z</mi><mi mathvariant="normal">∥</mi><mi>X</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathbb{P}(Z\|X)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathbb">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07153em">Z</span><span class="mord">∥</span><span class="mord mathnormal" style="margin-right:0.07847em">X</span><span class="mclose">)</span></span></span></span>.</li>
<li><em>Boundaries:</em> A linear classifier assumes that decision boundaries are linear.</li>
<li><em>Conditional independence:</em> A naive Bayes classifier assumes that the attribute values are independent of each other given the class.</li>
<li><em>Normally distributed:</em> Many statistical methods assume that data is normally distributed.</li>
</ol>
</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="ensembles">Ensembles<a href="#ensembles" class="hash-link" aria-label="Direct link to Ensembles" title="Direct link to Ensembles">​</a></h2>
<p>Ensemble methods combine multiple models to improve overall performance, robustness, and generalization capabilities. By leveraging the strengths of different models, ensemble techniques can often achieve better results than individual models (base learners). The effectiveness of an ensemble depends on the diversity and independence of the individual models: the less correlated the models, the better the ensemble performance. Thus, creating an ensemble with different architectures of models can significantly boost performance.</p>
<p>Although ensemble methods can provide substantial performance improvements, they are less favored in production due to their complexity in deployment and maintenance. However, they remain common in scenarios where even a small performance boost can lead to significant financial gains, such as predicting click-through rates for advertisements.</p>
<p><strong>Bagging</strong></p>
<p>Bagging, or <em>Bootstrap Aggregating</em>, aims to reduce variance and prevent overfitting by training multiple instances of the same model on different subsets of the training data. These subsets are generated by random sampling with replacement (bootstrap sampling). The predictions of these models are then aggregated, typically by averaging for regression tasks or majority voting for classification tasks.</p>
<p>Random Forests are a popular implementation of bagging, where multiple decision trees are trained on different subsets of the data and their predictions are averaged or voted upon to produce the final output. Random Forests also introduce additional randomness by selecting a random subset of features at each split in the trees, further reducing correlation among trees.</p>
<p><img decoding="async" loading="lazy" alt="Source: https://www.geeksforgeeks.org/bagging-vs-boosting-in-machine-learning/" src="/assets/images/bagging-7414313831953086430f5e4ea058ae87.png" width="1000" height="565" class="img_ev3q"></p>
<p><strong>Boosting</strong></p>
<p>Boosting aims to reduce bias and improve model accuracy by sequentially training models, where each new model attempts to correct the errors made by the previous models. This process focuses more on difficult instances that were misclassified or had higher errors in previous iterations. The final prediction is typically a weighted sum of the predictions from all models.</p>
<p>AdaBoost, short for <em>Adaptive Boosting</em>, assigns weights to each training instance and adjusts them after each iteration. Misclassified instances receive higher weights, forcing the next model to focus more on those hard-to-classify cases. The final model is a weighted sum of the individual models&#x27; predictions.</p>
<p>Gradient Boosting builds models sequentially, with each new model being trained to predict the residuals (errors) of the previous models. By minimizing these residuals, Gradient Boosting effectively improves the model&#x27;s accuracy over iterations. Gradient Boosting Machines (GBMs), such as XGBoost, LightGBM, and CatBoost, are popular implementations that offer efficient training and strong performance.</p>
<p><img decoding="async" loading="lazy" alt="Source: https://www.geeksforgeeks.org/bagging-vs-boosting-in-machine-learning/" src="/assets/images/boosting-4fbee477ecac7cfadca86df025cf8941.png" width="1000" height="561" class="img_ev3q"></p>
<p><strong>Stacking</strong></p>
<p>Stacking, or Stacked Generalization, involves training multiple base models and then using their predictions as inputs to a higher-level meta-model. The meta-model learns to combine the predictions and make the final predictions. The meta-model can be either a heuristic or another model. This approach allows for leveraging different types of models and their unique strengths.</p>
<p><img decoding="async" loading="lazy" alt="Source: https://supunsetunga.medium.com/stacking-in-machine-learning-357db1cfc3a" src="/assets/images/stacking-91af3443c0c4a8be13c98462d72ed82c.png" width="946" height="400" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="experiment-tracking-and-versioning">Experiment Tracking and Versioning<a href="#experiment-tracking-and-versioning" class="hash-link" aria-label="Direct link to Experiment Tracking and Versioning" title="Direct link to Experiment Tracking and Versioning">​</a></h2>
<p>Experiment tracking and versioning are practices in machine learning to ensure reproducibility, facilitate debugging, and manage the iterative nature of model development. By systematically tracking experiments you can compare different setups (architecture, hyperparameters, initialization, etc.) and better understand how changes affect your model&#x27;s performance.</p>
<p><strong>Experiment Tracking</strong></p>
<p>Experiment tracking involves recording all aspects of an experiment, including configurations, code, data, results, and metrics. This helps in comparing different experiments, understanding what changes lead to performance improvements, and ensuring reproducibility. A short list of things you might want to consider tracking for each experiment during its training:</p>
<ul>
<li>The <strong>configuration</strong> for the experiment, such as: hyperparameters, model architecture, data preprocessing steps, and other configuration settings.</li>
<li>The <strong>loss curve</strong> corresponding to the train split and each of the eval splits.</li>
<li>The <strong>model performance metrics</strong> such as accuracy, loss, precision, recall, F1 score, and any custom metrics relevant to the project.</li>
<li>The log of <strong>corresponding sample, prediction, and ground truth label</strong>. This comes in handy for ad hoc analytics and sanity checks.</li>
<li>The <strong>speed</strong> of your model, evaluated by the number of steps per second or, if your data is text, the number of tokens processed per second.</li>
<li><strong>System performance metrics</strong> such as memory usage and CPU/GPU utilization. They&#x27;re important to identify bottlenecks and avoid wasting system resources.</li>
</ul>
<p><strong>Versioning</strong></p>
<p>Versioning ensures that different versions of datasets, code, and models are systematically managed and can be reproduced or reverted to as needed.</p>
<p><strong>Debugging ML Models</strong></p>
<div class="theme-admonition theme-admonition-warning admonition_xJq3 alert alert--warning"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>Recommended Readings</div><div class="admonitionContent_BuS1"><p>For more comprehensive understanding of the topic, it&#x27;s recommended Andrej Karpathy&#x27;s blog post <a href="https://karpathy.github.io/2019/04/25/recipe/" target="_blank" rel="noopener noreferrer">A Recipe for Training Neural Networks</a></p></div></div>
<p>Some steps and strategies for debugging ML models according to Karpathy&#x27;s blog:</p>
<ol>
<li>
<p><strong>Become One with the Data</strong></p>
<ul>
<li>Spend significant time understanding and visualizing the data.</li>
<li>Look for patterns, imbalances, biases, and outliers.</li>
<li>Write code to filter, sort, and visualize data distributions and outliers.</li>
</ul>
</li>
<li>
<p><strong>Set Up End-to-End Training and Evaluation Skeleton</strong></p>
<ul>
<li>Start with a simple model (e.g., a linear classifier) to set up the training and evaluation pipeline.</li>
<li>Fix random seeds to ensure reproducibility.</li>
<li>Disable unnecessary features like data augmentation initially.</li>
<li>Verify that the initial loss and model behavior are as expected.</li>
<li>Use human-interpretable metrics and baselines for comparison.</li>
<li>Overfit a single batch to verify the model can learn properly.</li>
</ul>
</li>
<li>
<p><strong>Overfit</strong></p>
<ul>
<li>Initially, focus on overfitting a large model to the training data to ensure it can achieve a low error rate.</li>
<li>Choose a well-established model architecture related to your problem.</li>
<li>Use a forgiving optimizer like Adam with an appropriate learning rate.</li>
<li>Gradually introduce complexity and verify performance improvements.</li>
</ul>
</li>
<li>
<p><strong>Regularize</strong></p>
<ul>
<li>Once the model overfits, introduce regularization to improve validation accuracy.</li>
<li>Add more data if possible, as it is the best way to regularize.</li>
<li>Use data augmentation and pretrained models.</li>
<li>Reduce input dimensionality and model size if appropriate.</li>
<li>Apply techniques like dropout, weight decay, and early stopping.</li>
</ul>
</li>
<li>
<p><strong>Tune</strong></p>
<ul>
<li>Explore a wide range of hyperparameters using random search rather than grid search.</li>
<li>Consider hyperparameter optimization tools for more systematic tuning.</li>
</ul>
</li>
<li>
<p><strong>Squeeze Out the Juice</strong></p>
<ul>
<li>Once the best model and hyperparameters are found, use ensembles to boost performance.</li>
<li>Let models train longer than initially expected, as they often continue to improve.</li>
</ul>
</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="distributed-training">Distributed Training<a href="#distributed-training" class="hash-link" aria-label="Direct link to Distributed Training" title="Direct link to Distributed Training">​</a></h2>
<p>As models are getting bigger and more resource-intensive, companies care a lot more about training at scale. It&#x27;s common to train a model using data that doesn&#x27;t fit into memory. In these cases, our algorithms for preprocessing, shuffling, and batching data will need to run out-of-core and in parallel.</p>
<p>In some cases, a single data sample is so large it can&#x27;t fit into memory, and we&#x27;ll have to use something like gradient checkpointing. Even when a sample fits into memory, using checkpointing can allow you to fit more samples into a batch, which might allow you to train your model faster.</p>
<p><strong>Data Parallelism</strong></p>
<p>Data parallelism involves splitting the dataset into smaller chunks and distributing them across multiple devices (e.g., GPUs or nodes). Each device trains a copy of the model on its subset of the data, and you accumulate the gradients across devices to update the model parameters.</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>Synchronous Stochastic Gradient Descent (SGD)</div><div class="admonitionContent_BuS1"><p>In synchronous SGD, all devices wait until every device has completed its gradient computation for the current batch before averaging the gradients and updating the model parameters. This ensures that each device is working with the same model parameters at each step, leading to more stable convergence.</p></div></div>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>Asynchronous Stochastic Gradient Descent (SGD)</div><div class="admonitionContent_BuS1"><p>In asynchronous SGD, devices do not wait for each other to complete their gradient computations. Instead, each device independently updates the model parameters as soon as it finishes its computations. This approach can lead to faster training times as devices are not idly waiting, but it can introduce inconsistencies in the model parameters across devices, potentially affecting convergence stability.</p></div></div>
<p><img decoding="async" loading="lazy" alt="Source: https://www.oreilly.com/content/distributed-tensorflow/" src="/assets/images/data_parallel-7ac66c96713bfe5543060dbf83243a15.png" width="600" height="252" class="img_ev3q"></p>
<p>Asynchronous SGD theoretically converges with more steps than synchronous SGD. However, in practice, with a large number of weights, gradient updates are typically sparse, affecting only small fractions of the parameters. This reduces conflicts between updates from different machines. Consequently, gradient staleness is minimized, and the model converges similarly for both synchronous and asynchronous SGD.</p>
<p><strong>Model Parallelism</strong></p>
<p>Model parallelism involves splitting the model itself across multiple devices. Different parts of the model are assigned to different devices, and the forward and backward passes are executed across these devices. This approach is useful when the model is too large to fit into the memory of a single device.</p>
<p><img decoding="async" loading="lazy" alt="Source: https://papers.nips.cc/paper_files/paper/2012/hash/6aca97005c68f1206823815f66102863-Abstract.html" src="/assets/images/model_parallel_1-7f018bc9e88baaa0bbed90a17682a5e4.png" width="1226" height="1047" class="img_ev3q"></p>
<p><em>Pipeline parallelism</em> involves splitting the model into stages and distributing these stages across multiple devices. Each device processes a different part of the model, passing intermediate results to the next device in the pipeline.</p>
<p><img decoding="async" loading="lazy" alt="https://arxiv.org/abs/1811.06965" src="/assets/images/model_parallel_2-e868f13f78d01a8297c76f33067709bf.png" width="1579" height="667" class="img_ev3q"></p>
<p>In practice, combining both data parallelism and model parallelism can be beneficial, especially for very large models and datasets.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="automl">AutoML<a href="#automl" class="hash-link" aria-label="Direct link to AutoML" title="Direct link to AutoML">​</a></h2>
<p>Automated Machine Learning (AutoML) encompasses techniques and tools designed to automate parts of the machine learning pipeline. AutoML can significantly reduce the time and expertise required to build high-performing models by automating tasks such as hyperparameter tuning, feature selection, and model selection. AutoML can be broadly categorized into Soft AutoML and Hard AutoML.</p>
<p><strong>Soft AutoML: Hyperparameter tuning</strong></p>
<p>Hyperparameter tuning involves finding the best set of hyperparameters that maximize model performance. Soft AutoML focuses on automating this process to enhance efficiency and performance.</p>
<p>Key Techniques in Hyperparameter Tuning:</p>
<ul>
<li><strong>Grid Search:</strong> Exhaustively searches over a specified parameter grid to find the optimal hyperparameters. While thorough, it can be computationally expensive.</li>
<li><strong>Random Search:</strong> Randomly samples hyperparameter combinations within a specified range. It is more efficient than grid search and can often find good hyperparameters with fewer iterations.</li>
<li><strong>Bayesian Optimization:</strong> Uses a probabilistic model to predict the performance of hyperparameter combinations and iteratively improves this model to find the optimal set. This method balances exploration and exploitation, making it more efficient than random search.</li>
<li><strong>Hyperband:</strong> Combines random search with an early stopping strategy to allocate resources to promising hyperparameter configurations while discarding poor performers early.</li>
</ul>
<div class="theme-admonition theme-admonition-danger admonition_xJq3 alert alert--danger"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M5.05.31c.81 2.17.41 3.38-.52 4.31C3.55 5.67 1.98 6.45.9 7.98c-1.45 2.05-1.7 6.53 3.53 7.7-2.2-1.16-2.67-4.52-.3-6.61-.61 2.03.53 3.33 1.94 2.86 1.39-.47 2.3.53 2.27 1.67-.02.78-.31 1.44-1.13 1.81 3.42-.59 4.78-3.42 4.78-5.56 0-2.84-2.53-3.22-1.25-5.61-1.52.13-2.03 1.13-1.89 2.75.09 1.08-1.02 1.8-1.86 1.33-.67-.41-.66-1.19-.06-1.78C8.18 5.31 8.68 2.45 5.05.32L5.03.3l.02.01z"></path></svg></span>Danger</div><div class="admonitionContent_BuS1"><p>Never use your test split to tune hyperparameters. Choose the best set of hyperparameters for a model basedon a validation split, then report the model&#x27;s final performance on a test split.</p></div></div>
<p><strong>Hard AutoML: Architecture search and learned optimizer</strong></p>
<p>Hard AutoML involves more complex tasks, such as neural architecture search (NAS) and the development of learned optimizers, to automate the design and training of machine learning models.</p>
<p>A NAS setup consists of three components:</p>
<ol>
<li><strong>A Search Space:</strong> Defines possible model architectures, the building block to choose from and constraints on how they can be combined.</li>
<li><strong>A Performance Estimation Strategy:</strong> To evaluate the performance of a candidate architecture without having to train each candidate architecture from scratch until convergence.</li>
<li><strong>A Search Strategy:</strong> Some common approaches are random search, reinforcement learning (rewarding the choices that improve performance estimation) and evolution (adding mutations to an architecture, choosing the best-performing ones, and so on).</li>
</ol>
<p>Learned optimizers are trained on various tasks to develop effective optimization strategies that generalize across different problems. The creation process involves training a meta-optimizer on a diverse set of tasks, this meta-optimizer can then be applied to new tasks, improving convergence rates and overall model performance. The beauty of this approach is that this learned optimizer can then be used to train a better-learned optimizer.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="model-offline-evaluation">Model Offline Evaluation<a href="#model-offline-evaluation" class="hash-link" aria-label="Direct link to Model Offline Evaluation" title="Direct link to Model Offline Evaluation">​</a></h2>
<div class="theme-admonition theme-admonition-warning admonition_xJq3 alert alert--warning"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>Work in Progress</div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="baselines">Baselines<a href="#baselines" class="hash-link" aria-label="Direct link to Baselines" title="Direct link to Baselines">​</a></h3>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="evaluation-methods">Evaluation Methods<a href="#evaluation-methods" class="hash-link" aria-label="Direct link to Evaluation Methods" title="Direct link to Evaluation Methods">​</a></h3></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/ml-systems/chapter-5"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">5. Feature Engineering</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/ml-systems/chapter-7"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">7. Model Deployment and Prediction Service</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#six-tips-for-model-selection" class="table-of-contents__link toc-highlight">Six Tips for Model Selection</a></li><li><a href="#ensembles" class="table-of-contents__link toc-highlight">Ensembles</a></li><li><a href="#experiment-tracking-and-versioning" class="table-of-contents__link toc-highlight">Experiment Tracking and Versioning</a></li><li><a href="#distributed-training" class="table-of-contents__link toc-highlight">Distributed Training</a></li><li><a href="#automl" class="table-of-contents__link toc-highlight">AutoML</a></li><li><a href="#model-offline-evaluation" class="table-of-contents__link toc-highlight">Model Offline Evaluation</a><ul><li><a href="#baselines" class="table-of-contents__link toc-highlight">Baselines</a></li><li><a href="#evaluation-methods" class="table-of-contents__link toc-highlight">Evaluation Methods</a></li></ul></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">About</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/cv">Curriculum Vitae</a></li></ul></div><div class="col footer__col"><div class="footer__title">Reinforcement Learning</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/rl">Intro to RL</a></li></ul></div><div class="col footer__col"><div class="footer__title">Machine Learning</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/ml-systems">Designing Machine Learning Systems</a></li></ul></div><div class="col footer__col"><div class="footer__title">Statistics</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/statistics">Basic Statistical Concepts</a></li></ul></div><div class="col footer__col"><div class="footer__title">Contact</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/cruz-lucas" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://twitter.com/_Lucas_Cruz" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://linkedin.com/in/lucasdearaujocruz" target="_blank" rel="noopener noreferrer" class="footer__link-item">LinkedIn<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2024 Lucas Cruz. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>