<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-wiki/machine-learning/ml-systems/chapter-5" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.5.2">
<title data-rh="true">5. Feature Engineering | Lucas Cruz</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://lucas-cruz.com/ml-systems/chapter-5"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="5. Feature Engineering | Lucas Cruz"><meta data-rh="true" name="description" content="Feature engineering involves creating new features or transforming existing ones to improve the performance of models. Well-engineered features tend to give the models the biggest performance boost compared to algorithmic techniques such as hyperparameter tuning. This chapter explores the types of features, common feature engineering operations, strategies to avoid data leakage, and best practices for creating robust and generalizable features."><meta data-rh="true" property="og:description" content="Feature engineering involves creating new features or transforming existing ones to improve the performance of models. Well-engineered features tend to give the models the biggest performance boost compared to algorithmic techniques such as hyperparameter tuning. This chapter explores the types of features, common feature engineering operations, strategies to avoid data leakage, and best practices for creating robust and generalizable features."><link data-rh="true" rel="icon" href="/img/favicon/favicon.ico"><link data-rh="true" rel="canonical" href="https://lucas-cruz.com/ml-systems/chapter-5"><link data-rh="true" rel="alternate" href="https://lucas-cruz.com/ml-systems/chapter-5" hreflang="en"><link data-rh="true" rel="alternate" href="https://lucas-cruz.com/ml-systems/chapter-5" hreflang="x-default"><link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-MS3FF7B6JV"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-MS3FF7B6JV",{anonymize_ip:!0})</script>
<link rel="preconnect" href="https://www.googletagmanager.com">
<script>window.dataLayer=window.dataLayer||[]</script>
<script>!function(e,t,a,n){e[n]=e[n]||[],e[n].push({"gtm.start":(new Date).getTime(),event:"gtm.js"});var g=t.getElementsByTagName(a)[0],m=t.createElement(a);m.async=!0,m.src="https://www.googletagmanager.com/gtm.js?id=GTM-57WJMXBB",g.parentNode.insertBefore(m,g)}(window,document,"script","dataLayer")</script>



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.4c50bf2e.css">
<script src="/assets/js/runtime~main.6f7dc79e.js" defer="defer"></script>
<script src="/assets/js/main.efa2a684.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-57WJMXBB" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>

<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" target="_self" href="/"><div class="navbar__logo"><img src="/img/favicon/favicon-32x32.png" alt="Site Logo" class="custom-navbar-logo-class themedComponent_mlkZ themedComponent--light_NVdE" height="32" width="32"><img src="/img/favicon/dark-favicon-32x32.png" alt="Site Logo" class="custom-navbar-logo-class themedComponent_mlkZ themedComponent--dark_xIcU" height="32" width="32"></div><b class="navbar__title text--truncate">Lucas Cruz</b></a><div class="navbar__item dropdown dropdown--hoverable"><a class="navbar__link" aria-haspopup="true" aria-expanded="false" role="button" href="/wiki">Wiki</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/rl">Reinforcement Learning</a></li><li><a aria-current="page" class="dropdown__link dropdown__link--active" href="/ml-systems">Machine Learning</a></li><li><a class="dropdown__link" href="/statistics">Statistics</a></li></ul></div></div><div class="navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/cv">Curriculum Vitae</a><a href="https://github.com/cruz-lucas" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-github-link" aria-label="GitHub repository"></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" href="/ml-systems">Designing Machine Learning Systems</a><button aria-label="Collapse sidebar category &#x27;Designing Machine Learning Systems&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ml-systems/chapter-1">1. Overview of Machine Learning Systems</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ml-systems/chapter-2">2. Introduction to Machine Learning Systems Design</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ml-systems/chapter-3">3. Data Engineering Fundamentals</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ml-systems/chapter-4">4. Training Data</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/ml-systems/chapter-5">5. Feature Engineering</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ml-systems/chapter-6">6. Model Development and Offline Evaluation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ml-systems/chapter-7">7. Model Deployment and Prediction Service</a></li></ul></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/ml-systems"><span itemprop="name">Designing Machine Learning Systems</span></a><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">5. Feature Engineering</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Feature Engineering</h1></header>
<p>Feature engineering involves creating new features or transforming existing ones to improve the performance of models. Well-engineered features tend to give the models the biggest performance boost compared to algorithmic techniques such as hyperparameter tuning. This chapter explores the types of features, common feature engineering operations, strategies to avoid data leakage, and best practices for creating robust and generalizable features.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="learned-features-versus-engineered-features">Learned Features Versus Engineered Features<a href="#learned-features-versus-engineered-features" class="hash-link" aria-label="Direct link to Learned Features Versus Engineered Features" title="Direct link to Learned Features Versus Engineered Features">​</a></h2>
<p>In machine learning, features can be broadly categorized into two types: learned features and engineered features.</p>
<ul>
<li><strong>Learned Features:</strong> These are features automatically extracted by models, typically deep learning models, during the training process. Convolutional Neural Networks (CNNs) for image data and Large Language Models (LLMs) for text are examples where features are learned from the raw input data. However, not all features can be automatically learned.</li>
<li><strong>Engineered Features:</strong> These are features created manually based on domain-specific knowledge and insights into the data. Feature engineering involves transforming raw data into meaningful inputs that can improve the performance of machine learning algorithms.</li>
</ul>
<p>While learned features can capture complex patterns directly from raw data, engineered features leverage human expertise and can often lead to more interpretable models.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="common-feature-engineering-operations">Common Feature Engineering Operations<a href="#common-feature-engineering-operations" class="hash-link" aria-label="Direct link to Common Feature Engineering Operations" title="Direct link to Common Feature Engineering Operations">​</a></h2>
<p>This section presents some of the most common feature engineering operations, but is nowhere near being comprehensive.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="handling-missing-values">Handling Missing Values<a href="#handling-missing-values" class="hash-link" aria-label="Direct link to Handling Missing Values" title="Direct link to Handling Missing Values">​</a></h3>
<p>Missing values are a common issue in datasets and need to be addressed, but not all missing values are equal:</p>
<ul>
<li><strong>Missing not at random (MNAR):</strong> This is when the reason a value is missing is because of the true value itself. For instance, respondents to a form with higher income might be less prone to disclose it.</li>
<li><strong>Missing at random (MAR):</strong> This is when the reason a value is missing is not due to the value itself, but due to another observed variable. For instance, respondents from gender A might be less prone to disclosure their age.</li>
<li><strong>Missing completely at random (MCAR):</strong> This is when there&#x27;s no pattern in when the value is missing.</li>
</ul>
<p>There are two primary methods to handle missing values: deletion and imputation.</p>
<p><strong>Deletion</strong></p>
<ul>
<li><strong>Column deletion:</strong> Remove the feature that have high missing value rate. The drawback is that you can be removing important information and reducing the model&#x27;s accuracy.</li>
<li><strong>Row deletion:</strong> Remove the sample that has missing value(s). This method works best when the missing values are completely at random (MCAR) and the number of examples with missing values is small.</li>
</ul>
<p><strong>Imputation</strong></p>
<ul>
<li><strong>Default value:</strong> Fill the missing values with the default value. For example, if the job is missing, you might fill it with an empty string “&quot;.</li>
<li><strong>Mean, Median or Mode Imputation:</strong> Replace missing values with the mean, median, or mode (most frequent value) of the non-missing values. This method is simple but can introduce bias if the data is not normally distributed.</li>
<li><strong>Advanced Techniques:</strong> Use more sophisticated models like interpolation, regression, iterative imputation, or machine learning algorithms (such as KNN) to predict and fill in missing values.</li>
</ul>
<p>In general, you want to avoid filling missing values with possible values, such as the missing number of children with 0. It makes it hard to distinguish between people whose information is missing and people who don&#x27;t have children.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="scaling">Scaling<a href="#scaling" class="hash-link" aria-label="Direct link to Scaling" title="Direct link to Scaling">​</a></h3>
<p>Scaling transforms features to a common scale without distorting differences in the range of values. This is particularly important for algorithms sensitive to the scale of input data, such as Support Vector Machines (SVM), gradient-boosted trees, and logistic regression.</p>
<ul>
<li>
<p><strong>Min-Max Scaling:</strong> Scale features to a fixed range, typically <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[0, 1]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">[</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">1</span><span class="mclose">]</span></span></span></span>, but you can define arbitrary values <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mi>a</mi><mo separator="true">,</mo><mi>b</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[a, b]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">[</span><span class="mord mathnormal">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">b</span><span class="mclose">]</span></span></span></span> (i.e., <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mo>−</mo><mn>1</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[-1, 1]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">[</span><span class="mord">−</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">1</span><span class="mclose">]</span></span></span></span>).</p>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.25em" columnalign="center" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mover accent="true"><mi>x</mi><mo>~</mo></mover><mo>=</mo><mfrac><mrow><mi>x</mi><mo>−</mo><mi>min</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><mrow><mi>max</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>−</mo><mi>min</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mfrac></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi>o</mi><mi>r</mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mover accent="true"><mi>x</mi><mo>~</mo></mover><mo>=</mo><mi>a</mi><mo>+</mo><mfrac><mrow><mo stretchy="false">(</mo><mi>x</mi><mo>−</mo><mi>min</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">(</mo><mi>b</mi><mo>−</mo><mi>a</mi><mo stretchy="false">)</mo></mrow><mrow><mi>max</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>−</mo><mi>min</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mfrac></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{gather*}
  \tilde{x} = \frac{x - \min(x)}{\max(x) - \min(x)} \\
  or \\
  \tilde{x} = a + \frac{(x - \min(x))(b-a)}{\max(x) - \min(x)} \\
\end{gather*}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:6.826em;vertical-align:-3.163em"></span><span class="mord"><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:3.663em"><span style="top:-5.663em"><span class="pstrut" style="height:3.427em"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6679em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal">x</span></span><span style="top:-3.35em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.2222em"><span class="mord">~</span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mop">max</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mop">min</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mop">min</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span><span style="top:-3.587em"><span class="pstrut" style="height:3.427em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">or</span></span></span><span style="top:-1.5em"><span class="pstrut" style="height:3.427em"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6679em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal">x</span></span><span style="top:-3.35em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.2222em"><span class="mord">~</span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord mathnormal">a</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mop">max</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mop">min</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mop">min</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">))</span><span class="mopen">(</span><span class="mord mathnormal">b</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord mathnormal">a</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:3.163em"><span></span></span></span></span></span></span></span></span></span></span></span>
</li>
<li>
<p><strong>Standardization:</strong> Scale features to have a mean of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>x</mi><mo>ˉ</mo></mover></mrow><annotation encoding="application/x-tex">\bar{x}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5678em"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.5678em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal">x</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.2222em"><span class="mord">ˉ</span></span></span></span></span></span></span></span></span></span> and a standard deviation of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>σ</mi></mrow><annotation encoding="application/x-tex">\sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.03588em">σ</span></span></span></span>. If you think that your variable might follow a normal distribution, it might be helpful to normalize them with zero mean and unit variance.</p>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mover accent="true"><mi>x</mi><mo>~</mo></mover><mo>=</mo><mfrac><mrow><mi>x</mi><mo>−</mo><mover accent="true"><mi>x</mi><mo>ˉ</mo></mover></mrow><mi>σ</mi></mfrac></mrow><annotation encoding="application/x-tex">\tilde{x} = \frac{x-\bar{x}}{\sigma}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6679em"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6679em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal">x</span></span><span style="top:-3.35em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.2222em"><span class="mord">~</span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.9463em;vertical-align:-0.686em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.2603em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">σ</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.5678em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal">x</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.2222em"><span class="mord">ˉ</span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span>
</li>
<li>
<p><strong>Log Scaling:</strong> This method is useful for transforming skewed data to reduce the impact of outliers and make the distribution more normal. It involves taking the logarithm of the feature values.</p>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.25em" columnalign="center" columnspacing="0em"><mtr><mtd class="mtr-glue"></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mover accent="true"><mi>x</mi><mo>~</mo></mover><mo>=</mo><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd class="mtr-glue"></mtd><mtd class="mml-eqn-num"></mtd></mtr><mtr><mtd class="mtr-glue"></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi>o</mi><mi>r</mi></mrow></mstyle></mtd><mtd class="mtr-glue"></mtd><mtd class="mml-eqn-num"></mtd></mtr><mtr><mtd class="mtr-glue"></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mover accent="true"><mi>x</mi><mo>~</mo></mover><mo>=</mo><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>x</mi><mo>+</mo><mi>c</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd class="mtr-glue"></mtd><mtd class="mml-eqn-num"></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{gather}
  \tilde{x} = \log(x) \\
  or \\
  \tilde{x} = \log(x + c)
\end{gather}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:4.5em;vertical-align:-2em"></span><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.5em"><span style="top:-4.66em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6679em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal">x</span></span><span style="top:-3.35em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.2222em"><span class="mord">~</span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mop">lo<span style="margin-right:0.01389em">g</span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span><span style="top:-3.16em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">or</span></span></span><span style="top:-1.66em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6679em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal">x</span></span><span style="top:-3.35em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.2222em"><span class="mord">~</span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mop">lo<span style="margin-right:0.01389em">g</span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord mathnormal">c</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2em"><span></span></span></span></span></span></span></span><span class="tag"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.5em"><span style="top:-4.5em"><span class="pstrut" style="height:2.84em"></span><span class="eqn-num"></span></span><span style="top:-3em"><span class="pstrut" style="height:2.84em"></span><span class="eqn-num"></span></span><span style="top:-1.5em"><span class="pstrut" style="height:2.84em"></span><span class="eqn-num"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2em"><span></span></span></span></span></span></span></span></span>
<p>Where <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi></mrow><annotation encoding="application/x-tex">c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">c</span></span></span></span> is a constant added to ensure all values are positive before applying the logarithm.</p>
</li>
</ul>
<div class="theme-admonition theme-admonition-danger admonition_xJq3 alert alert--danger"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M5.05.31c.81 2.17.41 3.38-.52 4.31C3.55 5.67 1.98 6.45.9 7.98c-1.45 2.05-1.7 6.53 3.53 7.7-2.2-1.16-2.67-4.52-.3-6.61-.61 2.03.53 3.33 1.94 2.86 1.39-.47 2.3.53 2.27 1.67-.02.78-.31 1.44-1.13 1.81 3.42-.59 4.78-3.42 4.78-5.56 0-2.84-2.53-3.22-1.25-5.61-1.52.13-2.03 1.13-1.89 2.75.09 1.08-1.02 1.8-1.86 1.33-.67-.41-.66-1.19-.06-1.78C8.18 5.31 8.68 2.45 5.05.32L5.03.3l.02.01z"></path></svg></span>Attention</div><div class="admonitionContent_BuS1"><ol>
<li>Scaling is a major source of data leakage (covered in the next section).</li>
<li>It requires global statistics, calculated with training data, and saved to be used in test and inference. If the new data has changed significantly compared to the training, these statistics won&#x27;t be very helpful. Therefore, it&#x27;s important to retrain your model often to account for these changes.</li>
</ol></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="discretization">Discretization<a href="#discretization" class="hash-link" aria-label="Direct link to Discretization" title="Direct link to Discretization">​</a></h3>
<p>Discretization (or quantization) transforms continuous features into discrete buckets or bins. This can help models learn simpler representations and make them more interpretable. By converting continuous variables into discrete categories, we can enable models to treat similar feature values uniformly, potentially improving performance on certain tasks.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="encoding-categorical-features">Encoding Categorical Features<a href="#encoding-categorical-features" class="hash-link" aria-label="Direct link to Encoding Categorical Features" title="Direct link to Encoding Categorical Features">​</a></h3>
<p>Categorical features must be encoded into numerical values before being used in machine learning models. One challenge is that categories are not always static. For instance, if you treat product brands as categories, new brands can emerge that your model didn&#x27;t encounter during training. You can create an “UNKNOWN” category in your training data to prevent the model from crashing, but this approach treats all unseen brands, whether luxurious or sketchy, the same.</p>
<p><strong>The Hashing Trick</strong></p>
<p>One solution for handling dynamic categories is the <em>hashing trick</em>. A hash function generates a hashed value for each category, which becomes the index for that category. By specifying the hash space, you can fix the number of encoded values for a feature in advance, without needing to know the exact number of categories.</p>
<p>One problem with hashed functions is that different categories may hash to the same index. However, collisions are random and spread across the hash space. According to research done by Booking.com, even with 50% colliding features, the performance loss was less than 0.5%. <a href="https://booking.ai/dont-be-tricked-by-the-hashing-trick-192a6aae3087" target="_blank" rel="noopener noreferrer">Source</a></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="feature-crossing">Feature Crossing<a href="#feature-crossing" class="hash-link" aria-label="Direct link to Feature Crossing" title="Direct link to Feature Crossing">​</a></h3>
<p>Feature crossing creates new features by combining existing ones to capture interactions between features. This can help models learn non-linear relationships in the data. It&#x27;s helpful with model that are bad or even can&#x27;t learn non-linear representations, such as linear regression, logistic regression, and tree-based models. It&#x27;s less helpful in neural networks, but it could still help the model learn faster. DeepFM and xDeepFM are the family of models that have successfully leverage explicit feature interactions for recommender systems and click-through rate prediction.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="discrete-and-continuous-positional-embeddings">Discrete and Continuous Positional Embeddings<a href="#discrete-and-continuous-positional-embeddings" class="hash-link" aria-label="Direct link to Discrete and Continuous Positional Embeddings" title="Direct link to Discrete and Continuous Positional Embeddings">​</a></h3>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>Embedding</div><div class="admonitionContent_BuS1"><p>An embedding represents a piece of data as a vector. Word embeddings, for instance, map words to vectors in a continuous space. Similarly, positional embeddings map the position of each token in a sequence to a vector.</p></div></div>
<p>Introduced in the paper “Attention Is All You Need” (Vaswani et al., 2017), positional embeddings are essential for tasks in NLP and computer vision. They help models understand the order of inputs.</p>
<p>In models like transformers, words are processed in parallel, so positional information must be explicitly provided. We avoid using absolute positions (0, 1, 2, …) directly because neural networks don&#x27;t perform well with such inputs.</p>
<p><strong>Learned Position Embeddings</strong></p>
<p>One way to handle position embeddings is to treat it like we&#x27;d treat word embedding, by using an embedding matrix. Each position gets an embedding that is learned during training.</p>
<p><strong>Fixed Position Embeddings</strong></p>
<p>Another way to handle position embeddings is to use predefined functions, typically sine and cosine, to encode positions. This method, from the original Transformer paper, ensures positional embeddings capture relative positions effectively.</p>
<p>Fixed embeddings can be extended to continuous spaces using Fourier features, which are effective for tasks involving coordinates (or positions).</p>
<div class="theme-admonition theme-admonition-warning admonition_xJq3 alert alert--warning"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>Recommended Readings</div><div class="admonitionContent_BuS1"><ul>
<li>Tancik et al., <a href="https://arxiv.org/abs/2006.10739" target="_blank" rel="noopener noreferrer">Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains</a></li>
</ul></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="data-leakage">Data Leakage<a href="#data-leakage" class="hash-link" aria-label="Direct link to Data Leakage" title="Direct link to Data Leakage">​</a></h2>
<p>Data leakage occurs when information (i.e., the label) “leaks” from outside into the training dataset is used to create the model, leading to overly optimistic performance estimates during training but poor performance in production. It can happen because of how data are collected, handled, or even due to the innate origin of data (i.e., hospital A always sends patients with suspect of having lung cancer to a specific CT scan machine that outputs slightly different images).</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="common-causes-for-data-leakage">Common Causes for Data Leakage<a href="#common-causes-for-data-leakage" class="hash-link" aria-label="Direct link to Common Causes for Data Leakage" title="Direct link to Common Causes for Data Leakage">​</a></h3>
<ul>
<li><strong>Splitting time-correlated data randomly instead of by time:</strong> When working with time-series data (or time-correlated data, e.g., the time the data is generated affects its label distribution), splitting the data randomly can cause the model to learn from future information. Always split by time to ensure the model only has access to past information during training.</li>
<li><strong>Scaling before splitting:</strong> Performing scaling operations on the entire dataset before splitting can leak information from the test set into the training set. Scale the training data independently and apply the same transformation to the test set.</li>
<li><strong>Filling in missing data with statistics from the test split:</strong> Imputing missing values using statistics (e.g., mean, median) computed from the entire dataset can lead to leakage. Compute statistics only from the training data and use them to fill missing values in both training and test sets.</li>
<li><strong>Poor handling of data duplication before splitting:</strong> Duplicates in the dataset can cause leakage if not handled properly. Remove duplicates before splitting the data to ensure that no information from the test set influences the training process.</li>
<li><strong>Group leakage:</strong> When samples are grouped together, information from one sample can inadvertently leak into another if not split correctly. For example, in object detection task, photos taken milliseconds apart may land in different splits. Ensure that groups of related samples are kept entirely within either the training or test set.</li>
<li><strong>Leakage from data generation process:</strong> If the process used to generate the data unintentionally includes target information, it can cause leakage, just like in the CT scan machine example. Review the data generation process to ensure that no target information is inadvertently included in the features, and don&#x27;t forget to include subject matter experts, who have more contexts on how data is collected and used.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="detecting-data-leakage">Detecting Data Leakage<a href="#detecting-data-leakage" class="hash-link" aria-label="Direct link to Detecting Data Leakage" title="Direct link to Detecting Data Leakage">​</a></h3>
<p>Investigate the importance of each feature. If a feature has an unusually high importance that doesn&#x27;t make logical sense, it might be leaking information from the target. Keep an eye out the impact of new features on you model&#x27;s performance, if it improves a lot, the feature is very good or just contains leakage. Finally, be careful every time you look at the test split, only use it to report a model&#x27;s final performance.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="engineering-good-features">Engineering Good Features<a href="#engineering-good-features" class="hash-link" aria-label="Direct link to Engineering Good Features" title="Direct link to Engineering Good Features">​</a></h2>
<p>Effective feature engineering can significantly enhance the performance and robustness of machine learning models. This involves creating features that are both informative and generalizable while minimizing the number of features needed to train the model. By focusing on fewer, high-quality features, we reduce the risk of data leakage, decrease the likelihood of overfitting, and lower the memory requirements for serving the model. Additionally, this approach diminishes latency in feature extraction, particularly for online processing, and reduces technical debt, making the overall system more efficient and maintainable.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="feature-importance">Feature Importance<a href="#feature-importance" class="hash-link" aria-label="Direct link to Feature Importance" title="Direct link to Feature Importance">​</a></h3>
<ul>
<li><strong>Model-Based Methods:</strong> Algorithms like Random Forest, Gradient Boosting, and XGBoost provide built-in feature importance metrics. These methods assess the importance of each feature based on how often they are used to make splits in decision trees or their impact on the loss function.</li>
<li><strong>SHAP Values (SHapley Additive exPlanations):</strong> SHAP values provide a unified measure of feature importance by considering the contribution of each feature to every prediction made by the model.</li>
<li><strong>LIME (Local Interpretable Model-agnostic Explanations):</strong> LIME approximates the model locally around a prediction to understand the contribution of each feature.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="feature-generalization">Feature Generalization<a href="#feature-generalization" class="hash-link" aria-label="Direct link to Feature Generalization" title="Direct link to Feature Generalization">​</a></h3>
<p>Feature generalization ensures that the features used in the model are not overly specific to the training data and can generalize well to new, unseen data. Overall, there are two aspects to consider regarding generalization: feature coverage and the distribution of feature values.</p>
<ul>
<li>
<p><strong>Coverage:</strong> Feature coverage refers to the extent to which the features represent the entire input space. Ensuring broad coverage means that the model has seen a diverse range of examples during training, making it more likely to perform well on new data.</p>
</li>
<li>
<p><strong>Distribution:</strong> The distribution of feature values should be consistent between the training and test datasets. Features should not only cover the input space broadly but also follow the same statistical properties across different datasets.</p>
</li>
</ul></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/ml-systems/chapter-4"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">4. Training Data</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/ml-systems/chapter-6"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">6. Model Development and Offline Evaluation</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#learned-features-versus-engineered-features" class="table-of-contents__link toc-highlight">Learned Features Versus Engineered Features</a></li><li><a href="#common-feature-engineering-operations" class="table-of-contents__link toc-highlight">Common Feature Engineering Operations</a><ul><li><a href="#handling-missing-values" class="table-of-contents__link toc-highlight">Handling Missing Values</a></li><li><a href="#scaling" class="table-of-contents__link toc-highlight">Scaling</a></li><li><a href="#discretization" class="table-of-contents__link toc-highlight">Discretization</a></li><li><a href="#encoding-categorical-features" class="table-of-contents__link toc-highlight">Encoding Categorical Features</a></li><li><a href="#feature-crossing" class="table-of-contents__link toc-highlight">Feature Crossing</a></li><li><a href="#discrete-and-continuous-positional-embeddings" class="table-of-contents__link toc-highlight">Discrete and Continuous Positional Embeddings</a></li></ul></li><li><a href="#data-leakage" class="table-of-contents__link toc-highlight">Data Leakage</a><ul><li><a href="#common-causes-for-data-leakage" class="table-of-contents__link toc-highlight">Common Causes for Data Leakage</a></li><li><a href="#detecting-data-leakage" class="table-of-contents__link toc-highlight">Detecting Data Leakage</a></li></ul></li><li><a href="#engineering-good-features" class="table-of-contents__link toc-highlight">Engineering Good Features</a><ul><li><a href="#feature-importance" class="table-of-contents__link toc-highlight">Feature Importance</a></li><li><a href="#feature-generalization" class="table-of-contents__link toc-highlight">Feature Generalization</a></li></ul></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">About</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/cv">Curriculum Vitae</a></li></ul></div><div class="col footer__col"><div class="footer__title">Reinforcement Learning</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/rl">Intro to RL</a></li></ul></div><div class="col footer__col"><div class="footer__title">Machine Learning</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/ml-systems">Designing Machine Learning Systems</a></li></ul></div><div class="col footer__col"><div class="footer__title">Statistics</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/statistics">Basic Statistical Concepts</a></li></ul></div><div class="col footer__col"><div class="footer__title">Contact</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/cruz-lucas" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://twitter.com/_Lucas_Cruz" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://linkedin.com/in/lucasdearaujocruz" target="_blank" rel="noopener noreferrer" class="footer__link-item">LinkedIn<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2024 Lucas Cruz. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>