"use strict";(self.webpackChunklucas_cruz=self.webpackChunklucas_cruz||[]).push([[5328],{1749:(t,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>s,default:()=>p,frontMatter:()=>o,metadata:()=>c,toc:()=>a});var r=i(4848),n=i(8453);const o={title:"11. (WIP) Eligibility Traces",sidebar_position:11,slug:"/rl-intro-traces"},s="Eligibility Traces (WIP)",c={id:"wiki/reinforcement-learning/intro-to-rl/chapter-11",title:"11. (WIP) Eligibility Traces",description:"Working in Progress",source:"@site/docs/wiki/reinforcement-learning/1-intro-to-rl/chapter-11.mdx",sourceDirName:"wiki/reinforcement-learning/1-intro-to-rl",slug:"/rl-intro-traces",permalink:"/rl-intro-traces",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:11,frontMatter:{title:"11. (WIP) Eligibility Traces",sidebar_position:11,slug:"/rl-intro-traces"},sidebar:"RLSidebar",previous:{title:"9. On-policy Control with Approximation",permalink:"/rl-intro-approx-control"},next:{title:"12. Policy Gradient Methods",permalink:"/rl-intro-pg"}},l={},a=[];function d(t){const e={admonition:"admonition",h1:"h1",header:"header",p:"p",...(0,n.R)(),...t.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(e.header,{children:(0,r.jsx)(e.h1,{id:"eligibility-traces-wip",children:"Eligibility Traces (WIP)"})}),"\n",(0,r.jsx)(e.admonition,{type:"warning",children:(0,r.jsx)(e.p,{children:"Working in Progress"})})]})}function p(t={}){const{wrapper:e}={...(0,n.R)(),...t.components};return e?(0,r.jsx)(e,{...t,children:(0,r.jsx)(d,{...t})}):d(t)}},8453:(t,e,i)=>{i.d(e,{R:()=>s,x:()=>c});var r=i(6540);const n={},o=r.createContext(n);function s(t){const e=r.useContext(o);return r.useMemo((function(){return"function"==typeof t?t(e):{...e,...t}}),[e,t])}function c(t){let e;return e=t.disableParentContext?"function"==typeof t.components?t.components(n):t.components||n:s(t.components),r.createElement(o.Provider,{value:e},t.children)}}}]);