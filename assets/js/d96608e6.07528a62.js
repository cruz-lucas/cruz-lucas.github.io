"use strict";(self.webpackChunklucas_cruz=self.webpackChunklucas_cruz||[]).push([[5483],{1918:(t,o,n)=>{n.r(o),n.d(o,{assets:()=>s,contentTitle:()=>c,default:()=>f,frontMatter:()=>i,metadata:()=>l,toc:()=>p});var e=n(4848),r=n(8453);const i={title:"7.3 Off-policy Methods",sidebar_position:3,slug:"/rl/intro/func-approx/off-policy"},c="Off-policy Methods with Function Approximation",l={id:"wiki/reinforcement-learning/intro-to-rl/S&B-chapter-9-10-11/off-policy",title:"7.3 Off-policy Methods",description:"",source:"@site/docs/wiki/reinforcement-learning/1-intro-to-rl/S&B-chapter-9-10-11/off-policy.mdx",sourceDirName:"wiki/reinforcement-learning/1-intro-to-rl/S&B-chapter-9-10-11",slug:"/rl/intro/func-approx/off-policy",permalink:"/rl/intro/func-approx/off-policy",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:3,frontMatter:{title:"7.3 Off-policy Methods",sidebar_position:3,slug:"/rl/intro/func-approx/off-policy"},sidebar:"RLSidebar",previous:{title:"7.2 On-policy Control",permalink:"/rl/intro/func-approx/control"},next:{title:"8. Policy Gradient Methods",permalink:"/rl/intro/pg"}},s={},p=[];function a(t){const o={h1:"h1",header:"header",...(0,r.R)(),...t.components};return(0,e.jsx)(o.header,{children:(0,e.jsx)(o.h1,{id:"off-policy-methods-with-function-approximation",children:"Off-policy Methods with Function Approximation"})})}function f(t={}){const{wrapper:o}={...(0,r.R)(),...t.components};return o?(0,e.jsx)(o,{...t,children:(0,e.jsx)(a,{...t})}):a(t)}},8453:(t,o,n)=>{n.d(o,{R:()=>c,x:()=>l});var e=n(6540);const r={},i=e.createContext(r);function c(t){const o=e.useContext(i);return e.useMemo((function(){return"function"==typeof t?t(o):{...o,...t}}),[o,t])}function l(t){let o;return o=t.disableParentContext?"function"==typeof t.components?t.components(r):t.components||r:c(t.components),e.createElement(i.Provider,{value:o},t.children)}}}]);