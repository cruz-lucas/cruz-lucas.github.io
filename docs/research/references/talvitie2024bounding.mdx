---
title: Bounding-Box Inference for Error-Aware Model-Based Reinforcement Learning
slug: /research/talvitie2024bounding
---
import Flow from '@site/src/components/RL/Mdp'; 
import FlipCards from '@site/src/components/Card/FlipCards';

[Erin J Talvitie, Zilei Shao, Huiying Li, Jinghan Hu, Jacob Boerma, Rory Zhao, and Xintong Wang. "Bounding-Box Inference for Error-Aware Model-Based Reinforcement Learning." Reinforcement Learning Journal, vol. 1, no. 1, 2024, pp. TBD.](https://rlj.cs.umass.edu/2024/papers/Paper356.html)

:::warning

This content is a summary and includes my personal takeaways from the paper "[Bounding-Box Inference for Error-Aware Model-Based Reinforcement Learning](https://rlj.cs.umass.edu/2024/papers/Paper356.html)" by Talvitie et al. It reflects the points I found most relevant, along with my own conclusions on the topics discussed.

For a more comprehensive understanding of the topics and to align with the authors' perspectives, please **READ THE PAPER**.

:::

## Introduction

In model-based reinforcement learning (MBRL), an agent builds a model of the environment (_dynamics model_) to make decisions and predict future states. MBRL has the potential to be more sample-efficient, as the model can simulate experiences and enable the agent to learn more effectively from fewer real-world interactions. However, these approaches tend to overfit to model biases and cannot always rely on sufficiently accurate predictions to support planning.

To tackle this problem, the authors focus on _selective planning_, where the agent estimates the model's input-conditional accuracy and selectively uses the model when it is accurate. Then, they introduce _bounding-box
inference_, a novel method for measuring uncertainty over model-based updates to the value function.

## Problem Setting and Background

The problem is set on a traditional Markov Decision Process (MDP) formulation.

<Flow />

The key components are:

1. **Set of States ($S$):** The different scenarios or environment configuration the agent can encounter.
2. **Initial State ($s_0$):** The state where the agent begins, sampled from distribution $\mu$.
2. **Set of Actions ($A$):** The possible actions the agent can take.
3. **Transition Function ($Pr(S_{t+1} = s'| S_t = s, A_t = a)$):** This defines the probability of transitioning to a new state $s'$ given the current state $s$ and action $a$. The function is unkown to the agent.
4. **Reward Function ($R(s,a,s')$):** This assigns a reward based on the transition from state $s$ to state $s'$ via action $a$. The function is unkown to the agent.
6. **Discount Factor ($\gamma$):** This factor accounts for the preference of immediate rewards over future rewards. For example, if rewards are monetary, money available now can be invested to grow, making future money less valuable in comparison. Thus, future rewards are discounted.
7. **Policy ($\pi$):** A strategy used by the agent to decide actions based on states, so that $\pi(a|s)$ is the probability that $\pi$ chooses action $a$ in state $s$.
8. **State-action value ($q_\pi(s,a)$):** The expected cumulative reward when starting from state $s$, taking action $a$, and following policy $\pi$ thereafter. The expectation is over randomness from both the transition and policy distributions.

$$
q_\pi(a_t, s_t) = \mathbb E \bigg[\displaystyle\sum_{i=0}^\infty \gamma^{i-1} R_{t+i} | S_t = s_t, A_t = a_t \bigg]
$$


9. **Learned environment model ($\hat{p}, \hat{r}$):** A model that approximates the environment's dynamics. A common approach involves learning a _deterministic model_ (or _expectation model_). In this context, the model predicts the expected next state and reward for any given state-action pair. 

$$
\hat{p}(s_t,a_t) = s_{t+1}
$$

The authors assume that an agent is unable to learn a perfectly accurate environment model.

:::info Goal

The goal is for the agent to learn a policy that maximizes the state value for all $s$:

$$
v_\pi(s) = \mathbb E [q_\pi(s, A)]
$$

:::

### Sources of Model Error

This paper aims to reduce errors due to model inadequacy by using the model only in regions of the state space where it is known to provide reliable predictions.

<FlipCards />

### Model-Based Value Expansion

:::warning Work in Progress

:::