---
title: Meetings
slug: /research/meetings
---

## September 10, 2024

### Key Takeaways:

- Explore algorithms that deviate from traditional MBRL approaches, **we need to challenge established methods and assumptions**.

- Balance meetings with my coursework. I can either cancel some meetings temporarily or presenting only limited updates during busy periods.

- $\epsilon$-greedy strategy is very inefficient for GoRight environment.

- In the early 2000s: to translate results from bandit problems into RL, with model-based methods focusing on learning the model or making the correct decisions directly.

- The Rmax algorithm: states are considered "known" after sufficient visits, assigning Rmax rewards to unknown states, and planning accordingly.

- Many mistakes arise from inaccurate models, perfect model accuracy is rarely achieved.

- Model-free RL has also evolved, such as around 2008. Post 2015 (DQN), new algorithms like MuZero, which high capabilities but assumed a deterministic environment. The stochastic variant of MuZero did not gain traction.

- Dreamer represents a newer MBRL algorithm that delivers strong results but with notable brittleness when implementing.

- Experience replay in model-free RL can be viewed as a form of MBRL.

- Model-free RL might be missing opportunities for improved exploration by not fully utilizing planning capabilities. Planning, when done correctly, can guide exploration more effectively.

- A significant challenge is managing compounding errors over infinite horizons. Inaccurate models exacerbate this issue, we need robust methods to handle model inaccuracies during planning.

- Even with selective selective model value expansion (MVE), achieving the level of exploration guaranteed by Rmax remains elusive due to inherent model inaccuracies.


### Follow-up

- [x] Read [Erin's paper](https://rlj.cs.umass.edu/2024/papers/RLJ_RLC_2024_356.pdf)
- [x] Watching [Erin's talk](https://drive.google.com/file/d/1utgLnW9Rf0UuIpZnl_BopepM0XFSryaj/view)
- [x] Research journal
- [ ] Experiments repository
- [ ] Trying to reproduce some of the results. Understand the details. ASK WHY, justify every plot. (WIP)

Reproducing this plot:

![Average discounted rewards of Q learning and other simple model-based algorithms](@site/static/img/research/bbi/bbi_first_plot.png)

For that I need:
- [x] Environment
- [x] Q-learning agent (**sort of**)
- [ ] Perfect dynamics model (WIP)
- [ ] Model-based Q-learning Agent with value expansion rollouts (WIP)
- [ ] Expectation model
- [ ] Sample-based model



#### Training Algorithm
```
for N iterations:
    for 1 episode:
        for 500 steps:
            act randomly
            collect rewards
            update Q(S,A)
                Q(S,A) <- Q(S,A) + alpha [R + gamma max_a Q(S', a) - Q(S,A)]

    for 100 (independent) "episodes":
        for 500 steps:
            act greedly
            collect discounted returns
                G <- G + gamma^t R_{t+1}      
```

<span style={{"color":"red"}}>
This algorithm is wrong. We're measuring the variability of the training process, so we do the training loop 100 times instead of the evaluation. We evaluate with just 1 "episode" per iteration.
<br/>
</span>

#### Questions

**1. What is the number of training iterations?**

<span style={{"color":"red"}}>$$3\times10^5$$. Update 09/24/2024: the correct number of training iterations is $$600$$, each iteration has 500 interactions with the environment.<br/></span>

<span style={{"color":"red"}}>There is an inconsistency between the name of the x axis on this plot and on the previous on.</span>

![BBI final plots](@site/static/img/research/bbi/bbi_final_plot.png)

**2. What exactly is a point in the plot?**

<span style={{"color":"red"}}>Discounted return of an "episode". Update 09/24/2024: The returns throughout each episode, that is: $$G_0, G_1, \cdots, G_{499}, G_0, G_1, \cdots$$<br/></span>


**3. Does this mean that the agent rounds its observations? Or does the agent see without noise (the noisy observations would serve as a better training data for the dynamics model)?**
![Experimental setup description](@site/static/img/research/bbi/experimental_setup.png)

<span style={{"color":"red"}}>
The agent averages. The random offset is just to make sure the agent can use a continuous representation of the states.
</span>

#### Results so far

This are the last four "episodes" of the agent acting greedly, it is accumulating returns overtime. At the beggining of the episode it receives negative returns  until reach positive returns. The flat line means that the discount is so high that future reward doesn't interfere in the summation.

![Q-learning sort of working](@site/static/img/research/bbi/q_learning_sortofworking.png)

<span style={{"color":"red"}}>Good practice to plot the Q-values. Be the agent. Debbug the internals of the agent and environment.<br/></span>
<p float="center">
  <img src={require("@site/static/img/research/bbi/q_values_0.png").default} width="33%" />
  <img src={require("@site/static/img/research/bbi/q_values_5.png").default} width="33%" />
  <img src={require("@site/static/img/research/bbi/q_values_10.png").default} width="33%" />
</p>

#### Issues


<span style={{"color":"red"}}>Although I have implemented vectorized with numpy, this is taking too long. It shouldn't. Check with corrected algorithm and profile the code.<br/></span>
- My code is veeeery slow:
    - I didn't use multithreading or GPU
    - To get $3\times10^5$ training steps I would need ~60 hours


## September 17, 2024

All remarks related to the results presented are noted in red above.

### Key Takeaways:

- After fixing the algorithm, profiling the code to identify and understand performance bottlenecks. This initial analysis will help you pinpoint any areas where the code is behaving unexpectedly or inefficiently.

- When debugging, focus on running a single trial rather than multiple seeds. This allows you to quickly identify if things are working as expected without the overhead of unnecessary repetition.

- Instrument every aspect of your code that relates to debugging. Create a tight feedback loop so you can rapidly iterate and identify issues as they arise.

- For extensive experiments, particularly those lasting several minutes to over an hour, it's advisable to use Compute Canada. Reach out to Marcos for assistance in accessing and utilizing it.

- Always plot key internal metrics of the agent, such as Q-values. By incorporating as much data and as many measurements as possible, you can live debugg easier with peers.

- Develop a habit of structuring your code to automatically summarize all results. 

- Automate experiment management to reduce manual overhead and maintain consistency across trials. Tools like Weights & Biases (wandb) are highly recommended for tracking experiments, logging results, and visualizing progress.

### Follow-up

- [x] Experiments repository
- [ ] Trying to reproduce some of the results. Understand the details. ASK WHY, justify every plot. (WIP)
    - [x] Correct the algorithm
    - [x] Profile the code
    - [x] Track experiments
- [x] Gain access to Compute Canada

Reproducing this plot:

![Average discounted rewards of Q learning and other simple model-based algorithms](@site/static/img/research/bbi/bbi_first_plot.png)

For that I need:
- [x] Environment
- [x] Q-learning agent (**sort of**)
- [x] Perfect dynamics model
- [ ] Model-based Q-learning Agent with value expansion rollouts (WIP)
- [ ] Expectation model
- [ ] Sample-based model


#### Training Algorithm
```
for each seed:
    for N training steps:
        for 500 steps:
            act randomly
            collect rewards
            update Q(S,A)

        G <- 0
        for 500 steps:
            act greedly            
            collect discounted returns, t = 0, ..., 499
                G <- G + gamma^t R_{t+1}      

        log G
```

## September 24, 2024

### Key Takeaways:

- In the initial stages of research, it’s important to prioritize testing ideas quickly rather than optimizing early. The focus should be on experimenting, discarding, and iterating swiftly to evaluate different approaches.

- There was an error in the paper regarding the GoRight environment. Specifically, for Q-learning, the correct step size should be **0.05**, not **0.5**. Additionally, the total environment steps should be **300,000**, which equates to **600 "episodes"** of **500 steps each**.

- During graduate studies, it’s not necessary to make code public right away. Keep your repository private until it’s ready for release, and clean the commit history before publishing. This allows you to focus on what’s scientifically relevant without the pressure of sharing every discarded experiment.

- Similar to how you wouldn’t include every failed idea in a paper, you don’t need to show every discarded piece of code. Publicize only the polished, relevant work to encourage risk-taking and exploration in private.

- Students often want to write about the entire research journey, including failed attempts. However, advisors should ensure the final written work only focuses on what was actually implemented and successful. It’s fine to discuss the research path during talks, but the paper should highlight only the conclusive results.

- We discussed the idea of **MBVE** and its counterfactual approach, where the planning is greedy, but the policy remains random. There was also a mention of **maximization bias** in this context, I didn't fully undertand that to be honest.


### Follow-up

- [ ] Trying to reproduce some of the results. Understand the details. ASK WHY, justify every plot. (WIP)

Reproducing this plot:

![Average discounted rewards of Q learning and other simple model-based algorithms](@site/static/img/research/bbi/bbi_first_plot.png)

For that I need:
- [x] Environment
- [x] Q-learning agent (**sort of**)
- [x] Perfect dynamics model
- [ ] Model-based Q-learning Agent with value expansion rollouts (WIP)
- [ ] Expectation model
- [ ] Sample-based model

#### Q-learning agent in GoRight
![First results using Q-learning](@site/static/img/research/bbi/q_learning_results_1.png)

<span style={{"color":"red"}}>The agent converges faster and to a higher expected return. The hypothesis is that this is due to a bias in the initialization of the states when reseting the environment. The previous intensity status is set to 0 at every reset, it should be sampled.<br/></span>

The results for MVE are all zeros, I'm still debugging it because the Q-values look fine.

## October 01, 2024

### Key Takeaways:

- The Q-learning agent appears to converge faster and achieves a higher expected return than reported in the paper. The current hypothesis suggests this may be due to a **bias in state initialization** when resetting the environment. Specifically, the previous intensity status is always reset to **0**, when it should instead be **randomly sampled** at each reset to prevent bias.

- The results for MVE are consistently returning zeros, despite the Q-values appearing correct. Debugging is ongoing to identify the root cause. If I feel stuck, I should reach out to my advisors for support.

- Given the intensity of coursework in October, it’s important not to jeopardize academic performance for research. The priority should be on courses, and it's acceptable to drop the ball temporarily on research if necessary.

- It’s also fine to cancel meetings if I’m feeling overwhelmed.

- Overall, I’m making good progress for my first term, including interacting with the code, reproducing results, and developing an understanding of key issues (such as why the states should be randomized at each environment reset).


### Follow-up

- [ ] Trying to reproduce some of the results. Understand the details. ASK WHY, justify every plot. (WIP)

Reproducing this plot:

![Average discounted rewards of Q learning and other simple model-based algorithms](@site/static/img/research/bbi/bbi_first_plot.png)

For that I need:
- [x] Environment
- [x] Q-learning agent
- [x] Perfect dynamics model
- [ ] Model-based Q-learning Agent with value expansion rollouts (WIP)
- [ ] Expectation model
- [ ] Sample-based model