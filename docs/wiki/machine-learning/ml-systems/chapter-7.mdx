---
title: 7. Model Deployment and Prediction Service
sidebar_position: 7
slug: /ml-systems/chapter-7
---

# Model Deployment and Prediction Service


In many companies, the team that develops the models is also responsible for deploying them. In others, once the model is ready for deployment, it is exported and handed off to a different team (e.g., DevOps, MLOps, Data Platform) for deployment. This separation can lead to high communication overhead across teams and slow down model updates. It can also complicate debugging when issues arise.

:::tip Exporting a model
Exporting a model involves converting it into a format usable by another application, a process called "serialization." Two parts of a model can be serialized: the model definition and the model's parameter values. The model definition outlines the model's structure, while the parameter values (weights) provide the numerical values for its units.
:::

During development, our model runs in a controlled environment. For deployment, it can be moved to a staging environment for testing or directly to a production environment for use by end users.

The production environment can be as simple as wrapping a predict function in a POST request endpoint using frameworks like Flask or FastAPI. This involves building/installing all dependencies in a container alongside the model and hosting it on a cloud service like AWS or GCP. The exposed endpoint can then be utilized by downstream applications.

In this chapter, we'll discuss two main ways a model generates and serves its predictions to users: online and batch predictions. We will also cover where the computation should be performed: on the device (edge) or in the cloud. The method of serving and computing predictions influences the model's design, required infrastructure, and user experience.

## Machine Learning Deployment Myths

### Myth 1: You only deploy one or two ML models at a time

It's a common misconception, especially among those with an academic background, that organizations deploy only a few machine learning models at a time. Academic problems are often smaller in scale, solving one or few specific set of problems. In reality, companies often deploy hundreds or even thousands of models simultaneously. These models serve various purposes across different domains.

Consider a ride-sharing app like Uber. It needs to solve numerous problems, including ride demand prediction, driver availability, estimated time of arrival, dynamic pricing, fraud detection, customer churn, and more. Given that Uber operates in multiple countries, each with its unique market dynamics, the number of deployed models can be substantial. Managing such a large number of models requires robust infrastructure and careful orchestration to ensure they function correctly and efficiently.

### Myth 2: If we don't do anything, model performance remains the same

Another myth is that once a model is deployed, its performance remains stable over time without intervention. However, models can degrade due to several factors, including changes in data distribution shifts (data drift), evolving user behavior, and changes in the external environment. Regular monitoring, retraining, and updating of models are essential to maintain their performance and relevance.

### Myth 3: You don't need to update your models as much

Some believe that models, once deployed, need minimal updates. On the contrary, models often require frequent updates to adapt to new data and changing conditions. Continuous integration and continuous deployment (CI/CD) practices for ML models (referred to as MLOps) are critical to ensure models are consistently retrained, validated, and redeployed to maintain optimal performance. More on the frequency of retraining in Chapter 9. 

### Myth 4: Most ML engineers don't need to worry about scale

It's a misconception that scaling concerns are only relevant for a few ML engineers. In reality, most ML applications need to handle significant scale, whether it's the volume of data, the number of predictions made per second, or the complexity of the models. Engineers need to design and deploy models that can scale efficiently, manage large datasets, and serve predictions in real-time to meet business requirements.

## Batch Prediction versus Online Prediction

When deploying machine learning models, one fundamental decision you have to make involves choosing the mode of prediction. Each method serves different use cases and has its own advantages and trade-offs:

- Batch prediction, which uses only batch features.
- Online prediction that uses only batch features (e.g., precomputed embeddings).
- Online prediction that uses both batch features and streaming features. This is also known as streaming prediction.

### Batch Prediction
{/* 
**Batch prediction** refers to the process of generating predictions for a large set of data all at once, typically at scheduled intervals (e.g., daily, weekly) or when triggered. This approach is commonly used when predictions do not need to be generated in real-time but are still required for large volumes of data. Batch predictions is also known as _asynchronous prediction_.

**Key Characteristics:**
- **Efficiency:** Batch prediction is efficient for processing large volumes of data, as it allows for optimized use of resources. By processing data in bulk, it reduces the overhead associated with making individual predictions.
- **Use Cases:** It is well-suited for use cases such as generating product recommendations overnight for an e-commerce site, forecasting sales for the upcoming month, or scoring credit risk profiles for all customers at the end of each day.
- **Latency:** Batch prediction has higher latency since predictions are not generated instantly but are instead delivered after processing is complete. This is acceptable for tasks where real-time predictions are not required.
- **Infrastructure:** Requires a system capable of handling large datasets and often involves a pipeline to preprocess data, run predictions, and store the results in a database or data warehouse for further use.

**Advantages:**
- **Resource Optimization:** Can be scheduled during off-peak hours to take advantage of available computing resources.
- **Simplicity:** Easier to implement, as it doesnâ€™t require the real-time infrastructure necessary for online predictions.

**Challenges:**
- **Delayed Predictions:** Not suitable for applications that require immediate results.
- **Data Freshness:** Predictions might be based on outdated data if the batch job runs infrequently.

### Online Prediction

**Online prediction**, also known as real-time prediction, involves generating predictions on-the-fly as data is received. This method is critical for applications where immediate feedback or decision-making is necessary.

**Key Characteristics:**
- **Real-Time Response:** Online prediction provides immediate predictions as new data arrives, enabling real-time decision-making.
- **Use Cases:** Common in scenarios like fraud detection during a financial transaction, providing personalized content on a website, or estimating the time of arrival in a ride-sharing app.
- **Low Latency:** Requires very low latency, often measured in milliseconds, to ensure a seamless user experience.
- **Infrastructure:** Demands a robust, scalable infrastructure capable of handling potentially high volumes of requests in real-time without significant delays. This often includes the use of APIs, microservices, and caching mechanisms to speed up responses.

**Advantages:**
- **Immediate Feedback:** Ideal for scenarios where timely predictions are critical to the user experience or business outcomes.
- **Dynamic Adaptability:** Allows models to make predictions based on the most current data, improving the relevance and accuracy of predictions.

**Challenges:**
- **Complexity:** More challenging to implement and maintain due to the need for real-time data processing and robust infrastructure.
- **Resource Intensive:** Requires continuous availability of computing resources, which can be costly.

### Choosing Between Batch and Online Prediction

The choice between batch and online prediction depends largely on the specific requirements of the use case:

- **Batch prediction** is ideal when you can afford to process data at regular intervals and where predictions do not need to be immediate. It is also more cost-effective for handling large datasets in a controlled, predictable manner.
- **Online prediction** is necessary when real-time responses are critical, such as in applications where user interactions or transactions depend on instant feedback. It provides the most up-to-date predictions but at a higher operational cost and complexity.

In some cases, a hybrid approach might be used, where batch predictions are generated periodically, and online predictions are used to refine or update those predictions in real-time based on new data. This combination leverages the strengths of both approaches to meet the needs of complex, dynamic systems. */}