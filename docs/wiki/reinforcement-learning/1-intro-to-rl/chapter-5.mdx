---
title: 5. Temporal-Difference Learning
sidebar_position: 5
slug: /rl-intro-td
---

# Temporal-Difference Learning

:::info
In this chapter, you'll learn about:
- **Temporal-Difference (TD) Learning:** Combining ideas from Monte Carlo and Dynamic Programming methods.
- **TD Prediction and Control:** Estimating value functions and finding optimal policies incrementally and online.
- **TD(0) Algorithm:** The simplest form of TD learning for policy evaluation.
- **Advantages of TD Learning:** Understanding why TD methods can be more efficient and practical.
- **Comparing TD and Monte Carlo Methods:** Analyzing their performance and convergence properties.
:::

In previous chapters, we've explored **Dynamic Programming (DP)** methods, which require a complete model of the environment, and **Monte Carlo (MC)** methods, which learn from complete episodes but don't bootstrap. **Temporal-Difference (TD) Learning** bridges the gap between these two approaches by learning directly from raw experience without a model and updating estimates based partly on other learned estimates (bootstrapping). In this chapter, we'll see how TD methods learn state-values and policies incrementally and online.

## Introduction to Temporal-Difference Learning

**Temporal-Difference (TD) Learning** is a fundamental idea in reinforcement learning that combines concepts from both Monte Carlo methods and Dynamic Programming. Like Monte Carlo methods, TD learning can learn directly from raw experience without a model of the environment's dynamics. Like Dynamic Programming, TD methods update estimates based on other learned estimates (bootstrapping).

In TD learning, we update our value estimates based on the difference between consecutive estimates. The key idea is to use the **TD error**, which represents the difference between the predicted value and a better estimate obtained after observing the next reward and state.

#### TD(0) Algorithm

The simplest form of TD learning is the **TD(0)** algorithm, used for policy evaluation (prediction). The update rule for TD(0) is:

$$
V(S_t) \leftarrow V(S_t) + \alpha \left[ R_{t+1} + \gamma V(S_{t+1}) - V(S_t) \right]
$$

- $V(S_t)$ is the current estimate of the value of state $S_t$.
- $R_{t+1}$ is the reward received after taking action $A_t$ from state $S_t$.
- $\gamma$ is the discount factor.
- $\alpha$ is the step-size parameter (learning rate).
- The term in brackets is the **TD error** $\delta_t = R_{t+1} + \gamma V(S_{t+1}) - V(S_t)$.

In TD(0), the update at time $t$ uses the reward $R_{t+1}$ and the estimated value $V(S_{t+1})$ of the next state. This allows the agent to learn **online**, updating its estimates after each time step, without waiting for the end of an episode as in Monte Carlo methods.

The TD error $\delta_t$ measures the discrepancy between the predicted value $V(S_t)$ and the **target** value $R_{t+1} + \gamma V(S_{t+1})$. By minimizing this error over time, the value function estimates converge to the true values.

:::info Tabular TD(0) Prediction
$$
\begin{align*}
&\textbf{Input:}\text{ the policy } \pi \text{ to be evaluated} \\
&\textbf{Algorithm parameter:}\text{ step size } \alpha \in ( 0,1 ] \\
&\textbf{Initialize:}\\
&\qquad V(s), \text{ for all } s \in \mathcal{S}^+ \text{ arbitrarily except that } V(\text{terminal})=0\\[1em]

&\textbf{Loop for each episode:} \\
&\qquad \textbf{Initialize } S\\
&\qquad \textbf{Loop for each } \text{step of episode:}\\

&\qquad\qquad A \leftarrow \text{action given } \pi \text{ for } S \\
&\qquad\qquad \text{Take action } A \text{, observe } R,S' \\
&\qquad\qquad V(S) \leftarrow V(S) + \alpha [R+\gamma V(S') - V(S)] \\
&\qquad\qquad S \leftarrow S' \\
&\qquad \textbf{until } S \text{ is terminal} \\
\end{align*}
$$
:::

TD methods have several advantages due to their combination of bootstrapping (like DP) and sampling (like MC):

- TD methods do not require a model of the environment's dynamics.
- They can learn after each time step, making them suitable for continuous tasks.
- They often converge faster than Monte Carlo methods because they update estimates based on learned estimates rather than waiting for final outcomes.

Unlike Monte Carlo methods, which require complete episodes to compute returns, TD methods can learn from incomplete sequences. This makes TD methods applicable to non-episodic (continuing) tasks.

- **Variance:** TD methods typically have lower variance because they rely on bootstrapped estimates.
- **Bias:** TD methods may introduce bias due to bootstrapping but often achieve better performance in practice.
- **Efficiency:** TD methods can be more data-efficient, updating estimates more frequently.

## Sarsa: On-policy TD Control

:::info Sarsa (on-policy TD control)
$$
\begin{align*}

&\textbf{Algorithm parameter:}\text{ step size } \alpha \in ( 0,1 ], \text{small } \epsilon > 0 \\
&\textbf{Initialize:}\\
&\qquad Q(s,a), \text{ for all } s \in \mathcal{S}^+ \text{ arbitrarily except that } Q(\text{terminal}, \cdot)=0\\[1em]

&\textbf{Loop for each episode:} \\
&\qquad \textbf{Initialize } S\\
&\qquad \textbf{Choose } A \text{ from } S \text{ using policy derived from } Q (\text{e.g., } \epsilon \text{-greedy})\\
&\qquad \textbf{Loop for each } \text{step of episode}:\\

&\qquad\qquad \text{Take action } A \text{, observe } R,S' \\
&\qquad\qquad \textbf{Choose } A' \text{ from } S' \text{ using policy derived from } Q (\text{e.g., } \epsilon \text{-greedy})\\
&\qquad\qquad Q(S,A) \leftarrow Q(S,A) + \alpha [R+\gamma Q(S',A') - Q(S,A)] \\
&\qquad\qquad S \leftarrow S'; A \leftarrow A';\\
&\qquad \textbf{until } S \text{ is terminal} \\
\end{align*}
$$
:::


## Q-learning: Off-policy TD Control

:::info Q-learning (off-policy TD control)
$$
\begin{align*}

&\textbf{Algorithm parameter:}\text{ step size } \alpha \in ( 0,1 ], \text{small } \epsilon > 0 \\
&\textbf{Initialize:}\\
&\qquad Q(s,a), \text{ for all } s \in \mathcal{S}^+ \text{ arbitrarily except that } Q(\text{terminal}, \cdot)=0\\[1em]

&\textbf{Loop for each episode:} \\
&\qquad \textbf{Initialize } S\\
&\qquad \textbf{Loop for each } \text{step of episode:}\\

&\qquad\qquad \textbf{Choose } A \text{ from } S \text{ using policy derived from } Q (\text{e.g., } \epsilon \text{-greedy})\\
&\qquad\qquad Q(S,A) \leftarrow Q(S,A) + \alpha [R+\gamma \max_aQ(S',a) - Q(S,A)] \\
&\qquad\qquad S \leftarrow S'\\
&\qquad \textbf{until } S \text{ is terminal} \\
\end{align*}
$$
:::

## Expected Sarsa

## Maximization Bias and Double Learning

:::info Double Q-learning (off-policy TD control)
$$
\begin{align*}

&\textbf{Algorithm parameter:}\text{ step size } \alpha \in ( 0,1 ], \text{small } \epsilon > 0 \\
&\textbf{Initialize:}\\
&\qquad Q_1(s,a) \text{ and } Q_2(s,a),\\
&\qquad \text{for all } s \in \mathcal{S}^+ \text{ arbitrarily except that } Q(\text{terminal}, \cdot)=0\\[1em]

&\textbf{Loop for each episode:} \\
&\qquad \textbf{Initialize } S\\
&\qquad \textbf{Loop for each } \text{step of episode, }:\\

&\qquad\qquad \textbf{Choose } A \text{ from } S \text{ using the policy } \epsilon \text{-greedy in } Q_1+Q_2\\
&\qquad\qquad \textbf{Take action } A \text{, observe } R, S'\\
&\qquad\qquad \textbf{With } 0.5 \text{ probability:}\\
&\qquad\qquad\qquad Q_1(S,A) \leftarrow Q_1(S,A) + \alpha [R+\gamma Q_2(S', \argmax_a Q_1(S',a)) - Q_1(S,A)] \\
&\qquad\qquad \textbf{else:}\\
&\qquad\qquad\qquad Q_2(S,A) \leftarrow Q_2(S,A) + \alpha [R+\gamma Q_1(S', \argmax_a Q_2(S',a)) - Q_2(S,A)] \\
&\qquad\qquad S \leftarrow S'\\
&\qquad \textbf{until } S \text{ is terminal} \\
\end{align*}
$$
:::

## Recap

In this chapter, we've covered:
