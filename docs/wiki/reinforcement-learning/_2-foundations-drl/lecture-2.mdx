---
title: 2. Deep Q-Learning
sidebar_position: 2
slug: /drl-2
---

# Q-Learning

In the previous post, we laid the groundwork by exploring Markov Decision Processes (MDPs) and exact solution methods like value iteration and policy iteration. In this post, we'll build on those concepts into deep Q-learning.

## Recap of the Previous Post

In the first post, we covered the basics of optimal planning or control within an MDP framework. An MDP consists of a set of states, actions, a transition model, a reward function, a discount factor $\gamma$, and a horizon $H$. The objective is to find the optimal policy $\pi^*$ that maximizes expected rewards over time.

We introduced two exact methods to achieve this:
- **Value Iteration:** Iteratively updates the value of each state until convergence.
- **Policy Iteration:** Alternates between evaluating a policy and improving it until the optimal policy is found.

These methods come with limitations:

1. Both methods require knowledge of the environment's dynamics, i.e., the probability of transitioning to the next state given the current state and action. However, in many real-world scenarios, the agent doesn't have access to this information and must learn from interaction with the environment.

2. These methods involve looping over all possible states and actions. For complex environments, the number of states and actions can be enormous, making such exhaustive loops impractical.

Given these limitations, the focus of the remaining posts, including this one, will shift to more scalable approaches. Specifically, we'll explore how to approximate solutions using **sampling-based methods** and **function approximation** instead of relying on tabular methods.

- **Sampling-Based Methods:** The agent collects experiences by interacting with the environment and uses these experiences to approximate the optimal policy. This eliminates the requirement of knowing the dynamics model.

- **Function Approximation:** Instead of storing values or actions for every possible state in a table, we use a function, often represented by a neural network, to approximate these values or actions. This function can generalize across states, making it feasible to handle large or continuous state spaces.

## Recap of Q-Value

The optimal Q-value, denoted as $Q^*(s, a)$, represents the expected utility (sum of discounted rewards) an agent will accumulate when starting in state $s$, taking action $a$, and follow the optimal policy. The Q-value can be decomposed into two parts: the immediate reward obtained from taking action $a$ in state $s$, and the expected future rewards from the subsequent states and actions.

The Bellman equation for Q-values is given by:

$$
Q^*(s, a) = \sum_{s'} P(s' | s, a) \left( R(s, a, s') + \gamma \cdot \max_{a'} Q^*(s', a') \right)
$$

Here:
- $ P(s' | s, a) $ is the probability of transitioning to state $ s' $ given state $ s $ and action $ a $.
- $ R(s, a, s') $ is the reward obtained from this transition.
- $ \gamma $ is the discount factor, which prioritizes immediate rewards over future ones.
- $ \max_{a'} Q^*(s', a') $ captures the optimal action value from the next state onward.

### Q-Value Iteration

Q-value iteration is an iterative process that updates Q-values using the above Bellman equation. It starts with initial values (e.g., all zeros) and updates recursively:

$$
Q_{k+1}(s, a) \leftarrow \sum_{s'} P(s' | s, a) \left( R(s, a, s') + \gamma \cdot \max_{a'} Q_k(s', a') \right)
$$

This process repeats, using $ Q_k $ to find $ Q_{k+1} $, and converges to the optimal Q-values after sufficient iterations.

## (Tabular) Q-Learning

While Q-value iteration provides a direct way to compute optimal Q-values, it relies on knowing the exact transition probabilities ($ P(s' | s, a) $)‚Äîsomething rarely available in real-world scenarios. Q-learning addresses this by approximating these expectations through sampling, allowing the agent to learn directly from experience.

### Q-Learning Algorithm

First, we need to express the Q-value update as an expectation over possible next states:

    $$
    Q_{k+1}(s, a) \leftarrow \mathbb{E}_{s' \sim P(s' | s, a)} \left[ R(s, a, s') + \gamma \cdot \max_{a'} Q_k(s', a') \right]
    $$

This expectation can be approximated using samples collected from the agent's experience. For each state-action pair $(s, a)$, the agent observes the next state $ s' $, and the corresponding reward, then approximates the right-hand side using these samples:

    $$
    \text{target}(s') = R(s, a, s') + \gamma \cdot \max_{a'} Q_k(s', a')
    $$

And as we collect more samples, we update the Q-value by combining the old value with the new estimate from the sample. The update rule becomes:

    $$
    Q_{k+1}(s, a) \leftarrow (1 - \alpha) \cdot Q_k(s, a) + \alpha \cdot \text{target}(s')
    $$

    Here, $ \alpha $ is the learning rate, determining how much the new sample influences the Q-value update.

This running average gradually incorporates new experience into the Q-value estimates. Each time the agent revisits a state-action pair, it refines its Q-value based on new samples, leading to a more accurate approximation of the true expectation over time. Although each sample may be noisy or imprecise, the iterative averaging process helps the Q-values converge closer to the optimal values.

:::info Algorithm

Start with $Q^*_0(s,a)=0$ for all $s$, $a$.<br/>
Get initial state $s_0$.<br/>
For $k=1, 2, \cdots$, untill convergence:<br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Sample action $a$, get next state $s'$<br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If $s'$ is terminal:<br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\text{target} = R(s,a,s')$<br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Sample new initial state $s'$.

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;else:<br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\text{target} = R(s,a,s') + \gamma \max_{a'} Q_k(s', a')$<br/>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$Q_{k+1}(s, a) \leftarrow (1 - \alpha) \cdot Q_k(s, a) + \alpha \cdot \text{target}(s')$<br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$s \leftarrow s'$

:::


### How to sample actions?

- $\varepsilon$-Greedy: Choose random actions with probability $\color{orange}\varepsilon$ (Exploration), otherwise choose action greedily (Exploitation).

### Q-Learning Properties

- Q-Learning converges to optimal policy even if you're action suboptimally.
- This is called **off-policy learning**.
- Caveats:
    - You have to explore enough.
    - You have to eventually make learning rate ($\alpha$) small enough.
        - Otherwise, the latest experiences will make you hop around too much with every update.
    - ‚Ä¶ but not decrease too quickly.
        - You'll not update enough.
- Technical requirements:
    - All states and actions are visited infinitely often
        - Basically, in the limit, it doesn't matter how you select actions.
    - Learning rate schedule such that for all state and action pairs $(s,a)$:

$$
\displaystyle\sum_{t=0}^\infin \alpha_t(s,a)=\infin \qquad \qquad \displaystyle\sum_{t=0}^\infin \alpha_t^2(s,a)<\infin
$$

### Can Tabular Methods Scale?

Continuous (by crude discretization) and even discrete environments (such as tetris and Atari) have huge state space and it's not practical to have a table this large (tetris is $10^{60}$, humanoid is $10^{100}$).

### Approximate Q-Learning

Instead of a table, we have a ***parametrized Q function:*** $Q_\theta (s,a)$

- Can be a linear function in features.
- Or a neural net, decision tree, etc.

Learning rule:

- Remember:

$$
target(s') = R(s,a,s') + \gamma \cdot\underset{a'}{max}\ Q_{\theta_k}(s',a')
$$

- Update:

$$
\theta_{k+1} \larr \theta_k - \alpha \nabla_{\theta}\Bigg[\frac{1}{2}(Q_{\theta}(s,a) - target(s'))^2\Bigg]\Bigg\vert_{\theta=\theta_k}
$$

## Deep Q Networks (DQN)

![Source: https://doi.org/10.48550/arXiv.1312.5602](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/41cd0239-7573-4bb8-9ff4-3de4c2dc78ad/Screenshot_5.png)

Source: https://doi.org/10.48550/arXiv.1312.5602

![Screenshot_4.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/0c62fcdd-703e-4220-afa8-d70f82ab1d9b/Screenshot_4.png)

### DQN Details

- Uses Huber loss instead of squared loss on Bellman error.

$$
L_{\delta}=
\begin{cases}
    \frac{1}{2}a^2 & for |a|\le \delta, \\
    \delta(|a|-\frac{1}{2}\delta), & otherwise.
\end{cases}
$$

- Uses RMSProp instead of vanilla SGD.
- It helps to anneal the exploration rate.
    - Start $\varepsilon$ at 1 and anneal it to 0.1 or 0.05 over the first million frames.

### ATARI Network Architecture

- Convolutional neural network architecture:
    - History of frames as input.
    - One output per action - expected reward for that action $Q(s,a)$.
    - The final results used a slightly bigger network (3 convolutional  fully-connected hidden layer).

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/98c40c31-0cce-498b-a99f-a0eb8fa393ea/Untitled.png)

## Updates since Deepmind's DQN

### Double QDN

- There is an upward bias in $\underset{a}{max}\ Q(s,a;\theta)$.
- DQN maintains two sets of weights $\theta$ and $\theta^.$, so reduce bias by using:
    - $\theta$ for selecting the best action.
    - $\theta^.$ for evaluating the best action.

<aside>
üí° This helps with stabilizing the learning, it becomes a lot faster. It counterbalances the upward bias. In that way, there is some independence in how the action is chosen.

</aside>

- Double DQN loss:

$$
L_i(\theta_i)=\mathbb E_{s,a,s',r \ D}(r+\gamma \ Q(s', \underset{a'}{argmax}\ Q(s',a';\theta);\theta_i^-)-Q(s,a;\theta_i))^2
$$

### Prioritized Experience Replay

- Replaying all transitions with equal probability is highly suboptimal.
- Replay transitions in proportion to absolute Bellman error:

$$
\big|r + \gamma \ \underset{a'}{max} \ Q(s',a';\theta^-)-Q(s,a;\theta)\big|
$$

- leads to much faster learning.

<aside>
üí° There is a lot to learn when the target value is very different from what the $Q$ function predicted.

</aside>

### See also

- ‚ÄúRainbow: Combining Improvements in Deep Reinforcement Learning‚Äù, Matteo Hessel et al, 2017.
    - Double DQN (DDQN)
    - Prioritized Replay DDQN
    - Dueling DQN
    - Distributional DQN
    - Noisy DQN